services:
  proxy:
    image: proxy
    # image: ghcr.io/almat101/proxy:${IMAGE_TAG}
    build:
      context: ./proxy
      dockerfile: Dockerfile
    container_name: proxy
    restart: unless-stopped
    ports:
      - "80:80"
    depends_on:
      grafana:
        condition: service_healthy
      prometheus:
        condition: service_healthy
      node-exporter:
        condition: service_healthy
      alertmanager:
        condition: service_healthy
      auth-service:
        condition: service_healthy
      history-service:
        condition: service_healthy
      tournament-service:
        condition: service_healthy
    volumes:
      - media:/app/media:rw
      - proxy_logs:/var/log/nginx
      - ./proxy/frontend:/var/www/html:rw
    healthcheck:
      test: ["CMD-SHELL", "curl -f -k http://localhost:80 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
    networks:
      - internal

  auth-service:
    image: auth-service
    # image: ghcr.io/almat101/auth-service:${IMAGE_TAG}
    build:
      context: ./backend/auth_service
      dockerfile: Dockerfile
    container_name: auth-service
    command: gunicorn auth_service.wsgi:application --bind 0.0.0.0:8001 --workers=1 --threads=4
    env_file:
      - .env
    restart: unless-stopped
    volumes:
      - media:/app/media:rw
      - ./backend/auth_service/django:/app:rw
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:8001/watchman/?skip=watchman.checks.storage || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s
    depends_on:
      auth_db:
        condition: service_healthy
    networks:
      - internal

  auth_db:
    image: postgres:17-alpine3.22
    container_name: auth_db
    env_file: ".env"
    environment:
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_DB=${POSTGRES_AUTH_DB}
      - POSTGRES_HOST=${POSTGRES_AUTH_HOST}
    restart: always
    volumes:
      - postgres_auth:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U $$POSTGRES_USER -d $$POSTGRES_DB"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
    networks:
      - internal


  tournament-service:
    image: tournament-service
    # image: ghcr.io/almat101/tournament-service:${IMAGE_TAG}
    build:
      context: ./backend/tournament-service
      dockerfile: Dockerfile
    container_name: tournament-service
    command: gunicorn tournament.wsgi:application --bind 0.0.0.0:8003 --workers=1 --threads=4
    env_file:
      - .env
    restart: unless-stopped
    volumes:
      - ./backend/tournament-service:/app:rw
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:8003/watchman/ || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s
    depends_on:
      tournament_db:
        condition: service_healthy
    networks:
      - internal

  tournament_db:
    image: postgres:17-alpine3.22
    container_name: tournament_db
    env_file: ".env"
    environment:
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_DB=${POSTGRES_TOURNAMENT_DB}
      - POSTGRES_HOST=${POSTGRES_TOURNAMENT_HOST}
    restart: always
    volumes:
      - postgres_tournament:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U $$POSTGRES_USER -d $$POSTGRES_DB"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
    networks:
      - internal

  history-service:
    image: history-service
    # image: ghcr.io/almat101/history-service:${IMAGE_TAG}
    build:
      context: ./backend/history
      dockerfile: Dockerfile
    container_name: history-service
    command: gunicorn history_service.wsgi:application --bind 0.0.0.0:8002 --workers=1 --threads=4
    env_file:
      - .env
    restart: unless-stopped
    volumes:
      - ./backend/history:/app:rw
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:8002/watchman/ || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s
    depends_on:
      history_db:
        condition: service_healthy
    networks:
      - internal

  history_db:
    image: postgres:17-alpine3.22
    container_name: history_db
    env_file: ".env"
    environment:
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_DB=${POSTGRES_HISTORY_DB}
      - POSTGRES_HOST=${POSTGRES_HISTORY_HOST}
    restart: always
    volumes:
      - postgres_history:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U $$POSTGRES_USER -d $$POSTGRES_DB"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
    networks:
      - internal
  
  # cloudflared:
  #   image: cloudflare/cloudflared:2025.11.1
  #   container_name: cloudflared
  #   restart: unless-stopped
  #   command: tunnel --no-autoupdate run --token ${CLOUDFLARE_TUNNEL_TOKEN}
  #   networks:
  #     - internal

  #monitoring
  grafana:
    image: grafana/grafana:12.3
    container_name: grafana
    env_file: ".env"
    restart: unless-stopped
    environment:
      - GF_SERVER_ROOT_URL=https://localhost:80/grafana/
      - GF_SERVER_SERVE_FROM_SUB_PATH="true"
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana/dashboards:/var/lib/grafana/dashboards:ro
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning:ro
    depends_on:
      prometheus:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:3000/api/health || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    networks:
      - internal

  prometheus:
    image: prom/prometheus:v3.8.1
    container_name: prometheus
    restart: unless-stopped
    volumes:
      - prometheus_data:/prometheus/data
      - ./monitoring/prometheus/conf/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./monitoring/prometheus/rules:/etc/prometheus/rules:ro
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=10d'
      - '--web.external-url=https://localhost:80/prometheus/'
      - '--web.route-prefix=/'
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://localhost:9090/-/healthy || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    networks:
      - internal

  node-exporter:
    image: prom/node-exporter:v1.10.2
    container_name: node-exporter
    restart: unless-stopped
    depends_on:
      prometheus:
        condition: service_healthy
    command:
      - '--no-collector.thermal_zone'
      - '--collector.filesystem.ignored-mount-points=^/(sys|proc|dev|run)($|/)'
    healthcheck:
      test: ["CMD-SHELL", "wget --spider http://localhost:9100 || exit 1"]
      interval: 30s
      retries: 3
      start_period: 10s
      timeout: 10s
    networks:
      - internal

  alertmanager:
    image: prom/alertmanager:v0.30
    container_name: alertmanager
    env_file: ".env"
    restart: unless-stopped
    depends_on:
     - prometheus
    volumes:
      - alertmanager_data:/data
      - ./monitoring/alertmanager/tools/alert_email.sh:/etc/alertmanager/alert_email.sh:ro
    entrypoint: ["/bin/sh", "/etc/alertmanager/alert_email.sh"]
    command:
      - '--config.file=/etc/alertmanager/alertmanager.yml'
    healthcheck:
      test: ["CMD-SHELL", "wget --spider http://localhost:9093 || exit 1"]
      interval: 30s
      retries: 3
      start_period: 10s
      timeout: 10s
    networks:
      - internal

  nginx-prometheus-exporter:
    image: nginx/nginx-prometheus-exporter:1.4
    container_name: nginx-prometheus-exporter
    restart: unless-stopped
    depends_on:
      - proxy
    command: [
      "--nginx.scrape-uri", "http://proxy:8080/stub_status",
    ]
    networks:
      - internal

  postgres-exporter-auth-db:
    image: prometheuscommunity/postgres-exporter:v0.16.0
    container_name: postgres-exporter-auth-db
    environment:
      DATA_SOURCE_NAME: "postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@auth_db:5432/auth_db?sslmode=disable"
    restart: unless-stopped
    depends_on:
      prometheus:
        condition: service_healthy
      auth_db:
        condition: service_healthy
    networks:
      - internal

  postgres-exporter-tournament-db:
    image: prometheuscommunity/postgres-exporter:v0.16.0
    container_name: postgres-exporter-tournament-db
    environment:
      DATA_SOURCE_NAME: "postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@tournament_db:5432/tournament_db?sslmode=disable"
    restart: unless-stopped
    command:
      - '--web.listen-address=:9189'
    depends_on:
      prometheus:
        condition: service_healthy
      tournament_db:
        condition: service_healthy
    networks:
      - internal

  postgres-exporter-history-db:
    image: prometheuscommunity/postgres-exporter:v0.16.0
    container_name: postgres-exporter-history-db
    environment:
      DATA_SOURCE_NAME: "postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@history_db:5432/history_db?sslmode=disable"
    restart: unless-stopped
    command:
      - '--web.listen-address=:9190'
    depends_on:
      prometheus:
        condition: service_healthy
      history_db:
        condition: service_healthy
    networks:
      - internal


networks:
  internal:
    driver: bridge

volumes:
  proxy_logs:
  postgres_auth:
  postgres_tournament:
  postgres_history:
  media:
  #monitoring
  prometheus_data:
  alertmanager_data:
  grafana_data:
